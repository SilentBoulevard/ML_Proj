{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_str(st, numbers=False):\n",
    "    if type(st) == bytes:\n",
    "        try:\n",
    "            st = st.decode('utf-8').strip().lower()\n",
    "        except:\n",
    "            print('unicode error: {}'.format(st))\n",
    "\n",
    "    if numbers == True:\n",
    "        keep = set(string.ascii_lowercase + string.digits + string.punctuation + ' ')\n",
    "    else:\n",
    "        keep = set(string.ascii_lowercase + string.punctuation + ' ')\n",
    "\n",
    "    # clean string\n",
    "    st = ''.join(x if x in keep else ' ' for x in st)\n",
    "    # rem multiple spaces\n",
    "    st = re.sub(' +', ' ', st)\n",
    "\n",
    "    return st\n",
    "\n",
    "\n",
    "# mapper: cleanup a pd column or list of strings\n",
    "def cleanup_col(col, numbers=False):\n",
    "    col = map(lambda x: cleanup_str(x, numbers=numbers), col)\n",
    "    return list(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_tokenized(X, vocab_len):\n",
    "    binarizer = LabelBinarizer()\n",
    "    binarizer.fit(range(vocab_len))\n",
    "    X = np.array([binarizer.transform(x) for x in X])\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def char_preproc(X, Y, vocab_len, binarize=False):\n",
    "    # -----------------------------\n",
    "    # preproc X's------------------\n",
    "\n",
    "    # cleanup\n",
    "    X = cleanup_col(X, numbers=True)\n",
    "    # split in arrays of characters\n",
    "    #char_arrs = [[x for x in y] for y in X]\n",
    "\n",
    "    # tokenize\n",
    "    #tokenizer = Tokenizer(char_level=True)\n",
    "    tokenizer = Tokenizer(char_level=False)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    # token sequences\n",
    "    seq = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "    # pad to same length\n",
    "    #seq = pad_sequences(seq, maxlen=140, padding='post', truncating='post', value=0)\n",
    "    seq = pad_sequences(seq, maxlen=60, padding='post', truncating='post', value=0)\n",
    "\n",
    "    # make to on-hot\n",
    "    if binarize:\n",
    "        X = binarize_tokenized(seq, vocab_len)\n",
    "    else:\n",
    "        X = seq\n",
    "\n",
    "    # ----------------------------\n",
    "    # preproce Y's and return data\n",
    "\n",
    "    # one-hot encode Y's\n",
    "    Y = np.array([[1, 0] if x == 1 else [0, 1] for x in Y])\n",
    "\n",
    "    # generate and return final dataset\n",
    "    data = Dataset(X, Y, shuffle=True, testsize=0.02)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(load=True, binarize=False):\n",
    "    table = None\n",
    "\n",
    "    if os.path.isfile('data/processed/data-ready.pkl') and load:\n",
    "        print(\"data exists - loading\")\n",
    "\n",
    "        with open('data/processed/data-ready.pkl', 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "    else:\n",
    "        print(\"reading raw data and preprocessing..\")\n",
    "        table = pd.read_csv('data/rt-polarity.csv')\n",
    "        data = char_preproc(table.text, table.label, 70, binarize)\n",
    "\n",
    "        with open('data/processed/data-ready.pkl', 'wb') as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    return (data, table)\n",
    "\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, x, y=None, testsize=0.2, shuffle=False):\n",
    "\n",
    "        lend = len(x)\n",
    "\n",
    "        if testsize == None:\n",
    "            self.x_data = x\n",
    "            if y != None:\n",
    "                self.y_data = y\n",
    "\n",
    "            print('Single dataset of size {}'.format(lend))\n",
    "        else:\n",
    "            if shuffle:\n",
    "                si = np.random.permutation(np.arange(lend))\n",
    "                x = x[si]\n",
    "                y = y[si]\n",
    "                self.si = si\n",
    "\n",
    "            if type(testsize) == int:\n",
    "                testindex = testsize\n",
    "            else:\n",
    "                testindex = int(testsize * lend)\n",
    "\n",
    "            self.x_train = x[testindex:]\n",
    "            self.x_test = x[:testindex]\n",
    "            self.y_train = y[testindex:]\n",
    "            self.y_test = y[:testindex]\n",
    "            self.testindex = testindex\n",
    "\n",
    "            print('Train size: {}, test size {}'.format(len(self.y_train), len(self.y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2450000, test size 50000\n"
     ]
    }
   ],
   "source": [
    "with open('train_pos_full.txt') as f:\n",
    "    content_pos = f.readlines()\n",
    "    \n",
    "with open('train_neg_full.txt', errors='ignore') as g:\n",
    "    content_neg = g.readlines()\n",
    "    \n",
    "content = []\n",
    "content.extend(content_pos)\n",
    "content.extend(content_neg)\n",
    "X = content\n",
    "Y = [1] * len(content_pos) + [0] * len(content_neg)\n",
    "data = char_preproc(X, Y, vocab_len = 100, binarize = False)\n",
    "with open('/output/objs_word.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleanup_col(X, numbers=True)\n",
    "    # split in arrays of characters\n",
    "    #char_arrs = [[x for x in y] for y in X]\n",
    "\n",
    "    # tokenize\n",
    "    #tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer = Tokenizer(char_level=False)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "    # token sequences\n",
    "seq = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "    # pad to same length\n",
    "    #seq = pad_sequences(seq, maxlen=140, padding='post', truncating='post', value=0)\n",
    "seq = pad_sequences(seq, maxlen=50, padding='post', truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/output/objs_word.pkl', 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN SETTING\n",
      "IN DATA GENERATION\n",
      "input shape:  (2450000, 50)\n",
      "IN MODEL CREATION\n",
      "IN FORMALITIES\n",
      "logs/2017-12-21-01-07-02_embedding\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 50, 200)           20625400  \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 50, 950)           1520950   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 16, 950)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 16, 950)           7220950   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 8, 950)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 8, 950)            7220950   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 8, 950)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 8, 950)            7220950   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 7600)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              7783424   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 51,594,674\n",
      "Trainable params: 51,594,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "IN TRAINING\n",
      "Train on 2450000 samples, validate on 50000 samples\n",
      "Epoch 1/10\n",
      "2450000/2450000 [==============================] - 409s - loss: 0.3716 - acc: 0.8262 - val_loss: 0.3339 - val_acc: 0.8492\n",
      "Epoch 2/10\n",
      "2450000/2450000 [==============================] - 407s - loss: 0.3024 - acc: 0.8662 - val_loss: 0.3191 - val_acc: 0.8595\n",
      "Epoch 3/10\n",
      "2450000/2450000 [==============================] - 407s - loss: 0.2545 - acc: 0.8906 - val_loss: 0.3414 - val_acc: 0.8509\n",
      "Epoch 4/10\n",
      "2450000/2450000 [==============================] - 407s - loss: 0.1984 - acc: 0.9181 - val_loss: 0.4823 - val_acc: 0.8375\n",
      "Epoch 5/10\n",
      "2450000/2450000 [==============================] - 406s - loss: 0.1455 - acc: 0.9421 - val_loss: 0.5826 - val_acc: 0.8388\n",
      "Epoch 6/10\n",
      " 797000/2450000 [========>.....................] - ETA: 272s - loss: 0.1154 - acc: 0.9550"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from functions import *\n",
    "\n",
    "\n",
    "# conf and preprocess -----------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# settings ---------------------\n",
    "# ------------------------------\n",
    "print('IN SETTING')\n",
    "EMBEDDING = True\n",
    "TYPE = 'embedding' if EMBEDDING else 'standard'\n",
    "MODELPATH ='models/char-conv-' + TYPE + '-{epoch:02d}-{val_acc:.3f}-{val_loss:.3f}.hdf5'\n",
    "FILTERS = 500\n",
    "LR = 0.0001 if EMBEDDING else 0.00001\n",
    "\n",
    "CONV = [\n",
    "    {'filters':200, 'kernel':8, 'strides':1, 'padding':'same', 'reg': 0, 'pool':3},\n",
    "    {'filters':200, 'kernel':8, 'strides':1, 'padding':'same', 'reg': 0, 'pool':3},\n",
    "    {'filters':160, 'kernel':8, 'strides':1, 'padding':'same', 'reg': 0, 'pool':2},\n",
    "    {'filters':160, 'kernel':8, 'strides':1, 'padding':'same', 'reg': 0, 'pool':2},\n",
    "    {'filters':120, 'kernel':8, 'strides':1, 'padding':'same', 'reg': 0, 'pool':1},\n",
    "    {'filters':120, 'kernel':8, 'strides':1, 'padding':'same', 'reg': 0, 'pool':1},\n",
    "    {'filters':80, 'kernel':8, 'strides':1, 'padding':'same', 'reg': 0, 'pool':''},\n",
    "    {'filters':80, 'kernel':8, 'strides':1, 'padding':'same', 'reg': 0, 'pool':''}\n",
    "]\n",
    "\n",
    "CONV1 = [\n",
    "    {'filters':950, 'kernel':8, 'strides':1, 'padding':'same', 'reg': 0, 'pool':3},\n",
    "    {'filters':950, 'kernel':8, 'strides':1, 'padding':'same', 'reg': 0, 'pool':2},\n",
    "    {'filters':950, 'kernel':8, 'strides':1, 'padding':'same', 'reg': 0, 'pool':1},\n",
    "    {'filters':950, 'kernel':8, 'strides':1, 'padding':'same', 'reg': 0, 'pool':''}\n",
    "]\n",
    "\n",
    "# generate dataset -------------\n",
    "# ------------------------------\n",
    "print('IN DATA GENERATION')\n",
    "#data, table = load_processed_data(False, not EMBEDDING)\n",
    "\n",
    "print(\"input shape: \", np.shape(data.x_train))\n",
    "\n",
    "\n",
    "\n",
    "# model architecture ------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "# input and embedding ----------\n",
    "# ------------------------------\n",
    "print('IN MODEL CREATION')\n",
    "if EMBEDDING:\n",
    "\n",
    "    inputlayer = Input(shape=(50,))\n",
    "    network = Embedding(103127, 200, input_length=50)(inputlayer)\n",
    "\n",
    "else:\n",
    "    inputlayer = Input(shape=(140 ,70))\n",
    "    network = inputlayer\n",
    "\n",
    "# convolutional layers ---------\n",
    "# ------------------------------\n",
    "\n",
    "for C in CONV1:\n",
    "\n",
    "    # conv layer\n",
    "    network = Conv1D(filters=C['filters'], kernel_size=C['kernel'], \\\n",
    "                     strides=C['strides'], padding=C['padding'], activation='relu', \\\n",
    "                     kernel_regularizer=regularizers.l2(C['reg']))(network)\n",
    "\n",
    "    if type(C['pool']) != int:\n",
    "        continue\n",
    "\n",
    "    # pooling layer\n",
    "    network = MaxPooling1D(C['pool'])(network)\n",
    "\n",
    "# fully connected --------------\n",
    "# ------------------------------\n",
    "network = Flatten()(network)\n",
    "network = Dense(1024, activation='relu')(network)\n",
    "network = Dropout(0)(network)\n",
    "\n",
    "# output\n",
    "ypred = Dense(2, activation='softmax')(network)\n",
    "\n",
    "\n",
    "# training ----------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "# callbacks --------------------\n",
    "# ------------------------------\n",
    "\n",
    "# tensorboard\n",
    "print('IN FORMALITIES')\n",
    "TB_DIR = 'logs/' + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\") + '_' + TYPE\n",
    "\n",
    "os.makedirs(TB_DIR)\n",
    "tensorboard = TensorBoard(log_dir=TB_DIR)\n",
    "\n",
    "# early stopping and checkpoint\n",
    "estopping = EarlyStopping(monitor='val_acc', patience=1000)\n",
    "checkpoint = ModelCheckpoint(filepath=MODELPATH, save_best_only=True)\n",
    "\n",
    "# model-------------------------\n",
    "# ------------------------------\n",
    "\n",
    "optimizer = RMSprop(lr=LR)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputlayer, outputs=ypred)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(TB_DIR)\n",
    "print(model.summary())\n",
    "\n",
    "print('IN TRAINING')\n",
    "# fit and run ------------------\n",
    "# ------------------------------\n",
    "try:\n",
    "    hist = model.fit(data.x_train,\n",
    "                     data.y_train,\n",
    "                     validation_data=(data.x_test, data.y_test),\n",
    "                     epochs=10,\n",
    "                     batch_size=1000,\n",
    "                     shuffle=False,\n",
    "                     verbose=1,\n",
    "                     callbacks=[estopping, tensorboard])\n",
    "\n",
    "except KeyboardInterrupt:    \n",
    "    print(\"training stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('word.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test_data.txt') as h:\n",
    "    content_test = h.readlines()\n",
    "for i in range(len(content_test)):\n",
    "    content_test[i] = content_test[i].lstrip('1234567890,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = cleanup_col(content_test, numbers=True)\n",
    "    # split in arrays of characters\n",
    "#char_arrs = [[x for x in y] for y in X_test]\n",
    "\n",
    "    # tokenize\n",
    "tokenizer = Tokenizer(char_level=False)\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "\n",
    "    # token sequences\n",
    "seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    # pad to same length\n",
    "seq = pad_sequences(seq, maxlen=50, padding='post', truncating='post', value=0)\n",
    "X_test = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(X_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.zeros((ypred.shape[0],))\n",
    "count = 0\n",
    "for i in range(ypred.shape[0]):\n",
    "    if ypred[i][0] > ypred[i][1]:\n",
    "        y[i] = 1\n",
    "        count += 1\n",
    "    else:\n",
    "        y[i] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(y, columns=['Prediction'], index = range(1, 10001))\n",
    "sub.index.name = 'Id'\n",
    "sub = sub.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('sample_cnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('word.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from functions import *\n",
    "EMBEDDING = True\n",
    "TYPE = 'embedding' if EMBEDDING else 'standard'\n",
    "TB_DIR = 'logs/' + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\") + '_' + TYPE\n",
    "tensorboard = TensorBoard(log_dir=TB_DIR)\n",
    "\n",
    "# early stopping and checkpoint\n",
    "estopping = EarlyStopping(monitor='val_acc', patience=1000)\n",
    "try:\n",
    "    hist = model.fit(data.x_train,\n",
    "                     data.y_train,\n",
    "                     validation_data=(data.x_test, data.y_test),\n",
    "                     epochs=5,\n",
    "                     batch_size=250,\n",
    "                     shuffle=False,\n",
    "                     verbose=1,\n",
    "                     callbacks=[estopping, tensorboard])\n",
    "\n",
    "except KeyboardInterrupt:    \n",
    "    print(\"training stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('word.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
